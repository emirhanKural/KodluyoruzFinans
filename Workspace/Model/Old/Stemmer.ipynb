{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zemberek-python\n",
      "  Downloading zemberek_python-0.1.0-py3-none-any.whl (93.6 MB)\n",
      "Collecting antlr4-python3-runtime>=4.8\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "Requirement already satisfied: numpy>=1.18.2 in c:\\users\\enes_\\anaconda3\\lib\\site-packages (from zemberek-python) (1.18.5)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141234 sha256=6b02a162799a432d95d82c4036f01cc82a3c7f56bf82f2158bddfb873729cf4d\n",
      "  Stored in directory: c:\\users\\enes_\\appdata\\local\\pip\\cache\\wheels\\ca\\33\\b7\\336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, zemberek-python\n",
      "Successfully installed antlr4-python3-runtime-4.8 zemberek-python-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "###pip install zemberek-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Akbank_haber.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Endekste 101.000 sağlam destek</td>\n",
       "      <td>BIST 100, 103 bini aşağı geçtiğinde hedge şart...</td>\n",
       "      <td>15 Mart 2019</td>\n",
       "      <td>09:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analistlerden sanayi verisi yorumları geldi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14 Mart 2019</td>\n",
       "      <td>13:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bankalar uçarken havayolları iniyor</td>\n",
       "      <td>BOEING'in üretiminde kusurlu olup olmadığı inc...</td>\n",
       "      <td>14 Mart 2019</td>\n",
       "      <td>10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Şirket haberleri ve tavsiyeleri 13/3/2019</td>\n",
       "      <td>AKBNK, EREGL,THYAO, başta olmak üzere günün ön...</td>\n",
       "      <td>13 Mart 2019</td>\n",
       "      <td>08:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Şirket haberleri ve tavsiyeleri 12/3/2019</td>\n",
       "      <td>ALARK,EKGYO,GOLTS başta olmak üzere günün önem...</td>\n",
       "      <td>12 Mart 2019</td>\n",
       "      <td>09:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  \\\n",
       "0               Endekste 101.000 sağlam destek   \n",
       "1  Analistlerden sanayi verisi yorumları geldi   \n",
       "2          Bankalar uçarken havayolları iniyor   \n",
       "3    Şirket haberleri ve tavsiyeleri 13/3/2019   \n",
       "4    Şirket haberleri ve tavsiyeleri 12/3/2019   \n",
       "\n",
       "                                             content          date   time  \n",
       "0  BIST 100, 103 bini aşağı geçtiğinde hedge şart...  15 Mart 2019  09:27  \n",
       "1                                                NaN  14 Mart 2019  13:56  \n",
       "2  BOEING'in üretiminde kusurlu olup olmadığı inc...  14 Mart 2019  10:00  \n",
       "3  AKBNK, EREGL,THYAO, başta olmak üzere günün ön...  13 Mart 2019  08:59  \n",
       "4  ALARK,EKGYO,GOLTS başta olmak üzere günün önem...  12 Mart 2019  09:10  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "yedek = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=df.content[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rallide beklentilerin gerçekleşmesi ile kâr satışları için zemin müsait hale gelebilir. Endeksin 105 bin üzerinde kalması önem kazanacak'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title       0\n",
       "content    17\n",
       "date        0\n",
       "time        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'zemberek_grpc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d2e709137ed3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mzemberek_grpc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphology_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mz_morphology\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'zemberek_grpc'"
     ]
    }
   ],
   "source": [
    "import zemberek_grpc.morphology_pb2 as z_morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowballstemmer import TurkishStemmer\n",
    "turkStem=TurkishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ilişkilendiremedik\n",
      "kalemci\n",
      "gözlükçü\n",
      "gözle\n"
     ]
    }
   ],
   "source": [
    "print(turkStem.stemWord(\"ilişkilendiremediklerimiz\"))\n",
    "print(turkStem.stemWord(\"kalemci\"))\n",
    "print(turkStem.stemWord(\"gözlükçü\"))\n",
    "print(turkStem.stemWord(\"gözlem\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting JPype1\n",
      "  Downloading JPype1-1.0.2-cp37-cp37m-win_amd64.whl (1.6 MB)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in c:\\users\\enes_\\anaconda3\\lib\\site-packages (from JPype1) (3.7.4.2)\n",
      "Installing collected packages: JPype1\n",
      "Successfully installed JPype1-1.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install JPype1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zemberek.tokenization import TurkishTokenizer\n",
    "from zemberek.core.turkish import TurkishAlphabet, StemAndEnding, PrimaryPos\n",
    "from zemberek.core.text import TextUtil\n",
    "from zemberek.morphology.analysis.word_analysis import WordAnalysis\n",
    "from zemberek.morphology.analysis.rule_based_analyzer import RuleBasedAnalyzer\n",
    "from zemberek.morphology.analysis.unidentified_token_analyzer import UnidentifiedTokenAnalyzer\n",
    "from zemberek.morphology.generator import WordGenerator\n",
    "from zemberek.morphology.lexicon import RootLexicon\n",
    "from zemberek.morphology.morphotactics import TurkishMorphotactics, InformalTurkishMorphotactics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = TurkishTokenizer(\"benim adım enes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-188-bcbc7f75f9f3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-188-bcbc7f75f9f3>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    b*\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=a.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rallide beklentilerin gerçekleşmesi ile kâr satışları için zemin müsait hale gelebilir. Endeksin 105 bin üzerinde kalması önem kazanacak'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pd.Series(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           Rallide\n",
       "1     beklentilerin\n",
       "2     gerçekleşmesi\n",
       "3               ile\n",
       "4               kâr\n",
       "5         satışları\n",
       "6              için\n",
       "7             zemin\n",
       "8            müsait\n",
       "9              hale\n",
       "10       gelebilir.\n",
       "11         Endeksin\n",
       "12              105\n",
       "13              bin\n",
       "14         üzerinde\n",
       "15          kalması\n",
       "16             önem\n",
       "17        kazanacak\n",
       "dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     Endekste 101.000 sağlam destek\n",
       "1        Analistlerden sanayi verisi yorumları geldi\n",
       "2                Bankalar uçarken havayolları iniyor\n",
       "3          Şirket haberleri ve tavsiyeleri 13/3/2019\n",
       "4          Şirket haberleri ve tavsiyeleri 12/3/2019\n",
       "                            ...                     \n",
       "2075                     22 hisse için teknik analiz\n",
       "2076                 Kamu bankaları borsada düşüyor!\n",
       "2077    Akbank Direkt'in kârı 150 şube kârına ulaştı\n",
       "2078                 İşte Türkiye'den çıkacak devler\n",
       "2079            Türk bankaları için çok önemli uyarı\n",
       "Name: title, Length: 2080, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HARF BOYUTLARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     endekste 101.000 sağlam destek\n",
       "1        analistlerden sanayi verisi yorumları geldi\n",
       "2                bankalar uçarken havayolları iniyor\n",
       "3          şirket haberleri ve tavsiyeleri 13/3/2019\n",
       "4          şirket haberleri ve tavsiyeleri 12/3/2019\n",
       "                            ...                     \n",
       "2075                     22 hisse için teknik analiz\n",
       "2076                 kamu bankaları borsada düşüyor!\n",
       "2077    akbank direkt'in kârı 150 şube kârına ulaştı\n",
       "2078                i̇şte türkiye'den çıkacak devler\n",
       "2079            türk bankaları için çok önemli uyarı\n",
       "Name: title, Length: 2080, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title.apply(lambda x: \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-fba2a67f592c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m### missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4198\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4200\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-102-fba2a67f592c>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m### missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "df.content.apply(lambda x: \" \".join(x.lower() for x in x.split())) ### missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil = df.content.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sil.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sill = pd.DataFrame(sil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       bist 100, 103 bini aşağı geçtiğinde hedge şart...\n",
       "2       boeing'in üretiminde kusurlu olup olmadığı inc...\n",
       "3       akbnk, eregl,thyao, başta olmak üzere günün ön...\n",
       "4       alark,ekgyo,golts başta olmak üzere günün önem...\n",
       "5       hazine ve maliye bakanlığı 10 milyon tl sermay...\n",
       "                              ...                        \n",
       "2075    uzmanlar yirmiiki hisse için teknik analizde b...\n",
       "2076    kamu bankaları borsada özel bankalara göre neg...\n",
       "2077    hergün neredeyse 5000 yeni müşterinin tanıştığ...\n",
       "2078    türkiye’ye olan yabancı ilgisi kadar türkiye’d...\n",
       "2079    barclays'e göre türk bankalarında alım yapmak ...\n",
       "Name: content, Length: 2063, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sill.content.apply(lambda x: \" \".join(x.lower() for x in x.split())) ### missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOKTALAMA İŞARETLERİ SİLİNMESİ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "noktasiz_df = df.title.str.replace(\"[^\\w\\s]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     Endekste 101000 sağlam destek\n",
       "1       Analistlerden sanayi verisi yorumları geldi\n",
       "2               Bankalar uçarken havayolları iniyor\n",
       "3           Şirket haberleri ve tavsiyeleri 1332019\n",
       "4           Şirket haberleri ve tavsiyeleri 1232019\n",
       "                           ...                     \n",
       "2075                    22 hisse için teknik analiz\n",
       "2076                 Kamu bankaları borsada düşüyor\n",
       "2077    Akbank Direktin kârı 150 şube kârına ulaştı\n",
       "2078                 İşte Türkiyeden çıkacak devler\n",
       "2079           Türk bankaları için çok önemli uyarı\n",
       "Name: title, Length: 2080, dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noktasiz_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\enes_\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\enes_\\anaconda3\\lib\\site-packages (from nltk) (0.15.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\enes_\\anaconda3\\lib\\site-packages (from nltk) (4.46.1)\n",
      "Requirement already satisfied: click in c:\\users\\enes_\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\enes_\\anaconda3\\lib\\site-packages (from nltk) (2020.5.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stop-wordsNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
      "Building wheels for collected packages: stop-words\n",
      "  Building wheel for stop-words (setup.py): started\n",
      "  Building wheel for stop-words (setup.py): finished with status 'done'\n",
      "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32921 sha256=34dfb63f0194c853a1c60bc953183d41c6f403d177fe77fd7a5923c67c298d80\n",
      "  Stored in directory: c:\\users\\enes_\\appdata\\local\\pip\\cache\\wheels\\fb\\86\\b2\\277b10b1ce9f73ce15059bf6975d4547cc4ec3feeb651978e9\n",
      "Successfully built stop-words\n",
      "Installing collected packages: stop-words\n",
      "Successfully installed stop-words-2018.7.23\n"
     ]
    }
   ],
   "source": [
    "pip install stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\enes_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words(\"turkish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acaba',\n",
       " 'ama',\n",
       " 'aslında',\n",
       " 'az',\n",
       " 'bazı',\n",
       " 'belki',\n",
       " 'biri',\n",
       " 'birkaç',\n",
       " 'birşey',\n",
       " 'biz',\n",
       " 'bu',\n",
       " 'çok',\n",
       " 'çünkü',\n",
       " 'da',\n",
       " 'daha',\n",
       " 'de',\n",
       " 'defa',\n",
       " 'diye',\n",
       " 'eğer',\n",
       " 'en',\n",
       " 'gibi',\n",
       " 'hem',\n",
       " 'hep',\n",
       " 'hepsi',\n",
       " 'her',\n",
       " 'hiç',\n",
       " 'için',\n",
       " 'ile',\n",
       " 'ise',\n",
       " 'kez',\n",
       " 'ki',\n",
       " 'kim',\n",
       " 'mı',\n",
       " 'mu',\n",
       " 'mü',\n",
       " 'nasıl',\n",
       " 'ne',\n",
       " 'neden',\n",
       " 'nerde',\n",
       " 'nerede',\n",
       " 'nereye',\n",
       " 'niçin',\n",
       " 'niye',\n",
       " 'o',\n",
       " 'sanki',\n",
       " 'şey',\n",
       " 'siz',\n",
       " 'şu',\n",
       " 'tüm',\n",
       " 've',\n",
       " 'veya',\n",
       " 'ya',\n",
       " 'yani']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "\n",
    "stop_words = get_stop_words('turkish')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mu',\n",
       " 'onlar',\n",
       " 'seksen',\n",
       " 'ama',\n",
       " 'trilyon',\n",
       " 'buna',\n",
       " 'bizim',\n",
       " 'þeyden',\n",
       " 'yirmi',\n",
       " 'altý',\n",
       " 'iki',\n",
       " 'seni',\n",
       " 'doksan',\n",
       " 'dört',\n",
       " 'bunun',\n",
       " 'ki',\n",
       " 'nereye',\n",
       " 'altmýþ',\n",
       " 'hem',\n",
       " 'milyon',\n",
       " 'kez',\n",
       " 'otuz',\n",
       " 'beþ',\n",
       " 'elli',\n",
       " 'bizi',\n",
       " 'da',\n",
       " 'sekiz',\n",
       " 've',\n",
       " 'çok',\n",
       " 'bu',\n",
       " 'veya',\n",
       " 'ya',\n",
       " 'kýrk',\n",
       " 'onlarýn',\n",
       " 'ona',\n",
       " 'bana',\n",
       " 'yetmiþ',\n",
       " 'milyar',\n",
       " 'þunu',\n",
       " 'senden',\n",
       " 'birþeyi',\n",
       " 'dokuz',\n",
       " 'yani',\n",
       " 'kimi',\n",
       " 'þeyler',\n",
       " 'kim',\n",
       " 'neden',\n",
       " 'senin',\n",
       " 'yedi',\n",
       " 'niye',\n",
       " 'üç',\n",
       " 'þey',\n",
       " 'mý',\n",
       " 'tüm',\n",
       " 'onlari',\n",
       " 'bunda',\n",
       " 'ise',\n",
       " 'þundan',\n",
       " 'hep',\n",
       " 'þuna',\n",
       " 'bin',\n",
       " 'ben',\n",
       " 'ondan',\n",
       " 'kimden',\n",
       " 'bazý',\n",
       " 'belki',\n",
       " 'ne',\n",
       " 'bundan',\n",
       " 'gibi',\n",
       " 'de',\n",
       " 'onlardan',\n",
       " 'sizi',\n",
       " 'sizin',\n",
       " 'daha',\n",
       " 'niçin',\n",
       " 'þunda',\n",
       " 'INSERmi',\n",
       " 'bunu',\n",
       " 'beni',\n",
       " 'ile',\n",
       " 'þu',\n",
       " 'þeyi',\n",
       " 'sizden',\n",
       " 'defa',\n",
       " 'biz',\n",
       " 'için',\n",
       " 'dahi',\n",
       " 'siz',\n",
       " 'nerde',\n",
       " 'kime',\n",
       " 'birþey',\n",
       " 'birkez',\n",
       " 'her',\n",
       " 'biri',\n",
       " 'on',\n",
       " 'mü',\n",
       " 'diye',\n",
       " 'acaba',\n",
       " 'sen',\n",
       " 'en',\n",
       " 'hepsi',\n",
       " 'bir',\n",
       " 'bizden',\n",
       " 'sanki',\n",
       " 'benim',\n",
       " 'nerede',\n",
       " 'onu',\n",
       " 'benden',\n",
       " 'yüz',\n",
       " 'birkaç',\n",
       " 'çünkü',\n",
       " 'nasýl',\n",
       " 'hiç',\n",
       " 'katrilyon']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     Endekste 101.000 sağlam destek\n",
       "1        Analistlerden sanayi verisi yorumları geldi\n",
       "2                Bankalar uçarken havayolları iniyor\n",
       "3             Şirket haberleri tavsiyeleri 13/3/2019\n",
       "4             Şirket haberleri tavsiyeleri 12/3/2019\n",
       "                            ...                     \n",
       "2075                          22 hisse teknik analiz\n",
       "2076                 Kamu bankaları borsada düşüyor!\n",
       "2077    Akbank Direkt'in kârı 150 şube kârına ulaştı\n",
       "2078                 İşte Türkiye'den çıkacak devler\n",
       "2079                     Türk bankaları önemli uyarı\n",
       "Name: title, Length: 2080, dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title.apply(lambda x: \" \".join(x for x in x.split() if x not in sw and stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Az Geçen Kelimelerin Silinmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "için            287\n",
       "ve              214\n",
       "Akbank          192\n",
       "haberleri       167\n",
       "hisse           158\n",
       "               ... \n",
       "Akbank'ı          1\n",
       "piyasalar         1\n",
       "Deutsche'nin      1\n",
       "yapın             1\n",
       "durun             1\n",
       "Length: 3431, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(\" \".join(df[\"title\"]).split()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yenilik         1\n",
       "Şahenk          1\n",
       "kapatabilir     1\n",
       "hakkında        1\n",
       "Birimi          1\n",
       "1000’ler        1\n",
       "1,2             1\n",
       "Fest            1\n",
       "ardından        1\n",
       "yapabilir       1\n",
       "‘1              1\n",
       "Robotlarla      1\n",
       "mü              1\n",
       "markasını       1\n",
       "CEO’su          1\n",
       "12,3            1\n",
       "FOMC            1\n",
       "İçerde          1\n",
       "\"seçici         1\n",
       "Merrill         1\n",
       "Finance'tan     1\n",
       "Finansbank      1\n",
       "rekabet         1\n",
       "iştahını        1\n",
       "müşterileri     1\n",
       "Akbank'ı        1\n",
       "piyasalar       1\n",
       "Deutsche'nin    1\n",
       "yapın           1\n",
       "durun           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(\" \".join(df[\"title\"]).split()).value_counts()[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "silinecekler = pd.Series(\" \".join(df[\"title\"]).split()).value_counts()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       BIST 100, 103 bini aşağı geçtiğinde hedge şart...\n",
       "2       BOEING'in üretiminde kusurlu olup olmadığı inc...\n",
       "3       AKBNK, EREGL,THYAO, başta olmak üzere günün ön...\n",
       "4       ALARK,EKGYO,GOLTS başta olmak üzere günün önem...\n",
       "5       Hazine ve Maliye Bakanlığı 10 milyon TL sermay...\n",
       "                              ...                        \n",
       "2075    Uzmanlar yirmiiki hisse için teknik analizde b...\n",
       "2076    Kamu bankaları borsada özel bankalara göre neg...\n",
       "2077    Hergün neredeyse 5000 yeni müşterinin tanıştığ...\n",
       "2078    Türkiye’ye olan yabancı ilgisi kadar Türkiye’d...\n",
       "2079    Barclays'e göre Türk bankalarında alım yapmak ...\n",
       "Name: content, Length: 2063, dtype: object"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sill.content.apply(lambda x: \" \".join(x for x in x.split() if x not in silinecekler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\enes_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\enes_\\anaconda3\\lib\\site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\enes_\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\enes_\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (0.15.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\enes_\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.46.1)\n",
      "Requirement already satisfied: regex in c:\\users\\enes_\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2020.5.14)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     [Endekste, 101.000, sağlam, destek]\n",
       "1       [Analistlerden, sanayi, verisi, yorumları, geldi]\n",
       "2                [Bankalar, uçarken, havayolları, iniyor]\n",
       "3         [Şirket, haberleri, ve, tavsiyeleri, 13/3/2019]\n",
       "4         [Şirket, haberleri, ve, tavsiyeleri, 12/3/2019]\n",
       "                              ...                        \n",
       "2075                    [22, hisse, için, teknik, analiz]\n",
       "2076                  [Kamu, bankaları, borsada, düşüyor]\n",
       "2077    [Akbank, Direkt'in, kârı, 150, şube, kârına, u...\n",
       "2078                 [İşte, Türkiye'den, çıkacak, devler]\n",
       "2079          [Türk, bankaları, için, çok, önemli, uyarı]\n",
       "Name: title, Length: 2080, dtype: object"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"].apply(lambda x: TextBlob(x).words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowballstemmer import TurkishStemmer\n",
    "stem=TurkishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 Endeks 101.000 sağla destek\n",
       "1              Analist sanayi veris yorum gel\n",
       "2                   Banka uçar havayol iniyor\n",
       "3           Şirket haber ve tavsiye 13/3/2019\n",
       "4           Şirket haber ve tavsiye 12/3/2019\n",
       "                        ...                  \n",
       "2075                  22 his iç teknik analiz\n",
       "2076                Kamu banka borsa düşüyor!\n",
       "2077    Akbank Direkt' kârı 150 şube kâr ulaş\n",
       "2078                İşte Türkiye' çıkacak dev\n",
       "2079            Türk banka iç çok önemli uyar\n",
       "Name: title, Length: 2080, dtype: object"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title.apply(lambda x:\" \".join([stem.stemWord(i) for i in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zemberek.morphology.analysis.word_analysis import WordAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
